---
title: Neurol ODE 原文翻译
categories:
  - work
tags:
  - paper
createTime: 2025/03/04 11:33:07
permalink: /posts/neurolode/
---

> 原文：[Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366)

> 文字讲解：
> - [神经常微分方程 (Neural ODE)：入门教程](https://zhuanlan.zhihu.com/p/621912107)
> - [Neural Ordinary Differential Equations神经常微分方程总结](https://zhuanlan.zhihu.com/p/658476990)
> 视频讲解：
> - [Neural ODE论文分享](https://www.bilibili.com/video/BV1Kc411n7SA/?share_source=copy_web&vd_source=7b952f197435de82efe1dabbfc57b22b)
> - [Neural Ordinary Differential Equations 《神经常微分方程》]( https://www.bilibili.com/video/BV1WG41137gZ/?share_source=copy_web&vd_source=7b952f197435de82efe1dabbfc57b22b)



## Abstract

我们介绍了一类新的深度神经网络模型。我们不再指定一系列离散的隐藏层，而是通过神经网络来参数化隐藏状态的导数。网络的输出通过一个黑箱微分方程求解器进行计算。这些连续深度模型具有恒定的内存成本，能够根据每个输入自适应评估策略，并且可以明确地在数值精度与速度之间进行权衡。我们在连续深度残差网络和连续时间潜变量模型中展示了这些特性。我们还构建了连续归一化流，这是一种生成模型，可以通过最大似然训练，而无需对数据维度进行分区或排序。在训练方面，我们展示了如何在任何ODE求解器中可扩展地进行反向传播，而无需访问其内部操作。这使得在更大模型中实现ODE的端到端训练成为可能。

## 1. Introduction

像残差网络、递归神经网络解码器和归一化流这样的模型，通过组合一系列变换来构建复杂的变换，生成一个隐藏状态的序列：

$$
h_{t+1} = h_t + f(h_t, \theta_t) \tag{1}
$$

其中  $t \in \{0 \dots T\}$  且  $h_t \in \mathbb{R}^D$ 。这些迭代更新可以被看作是连续变换的欧拉离散化（Lu et al., 2017; Haber and Ruthotto, 2017; Ruthotto and Haber, 2018）。

当我们增加更多的层并采取更小的步骤时，会发生什么？在极限情况下，我们通过一个神经网络来参数化隐藏单元的连续动态，这些动态由一个普通的微分方程（ODE）指定：

$$
\frac{dh(t)}{dt} = f(h(t), t, \theta) \tag{2}
$$

从输入层  $h(0)$  开始，我们可以定义输出层  $h(T)$  为该微分方程初值问题在某个时间  $T$  的解。这个值可以通过一个黑箱微分方程求解器计算，该求解器在需要的地方评估隐藏单元动态  $f$ ，以确定具有所需精度的解。图1对比了这两种方法。

使用微分方程求解器定义和评估模型具有几个优点：

**内存效率**  
在第二节中，我们展示了如何计算标量值损失相对于任何ODE求解器所有输入的梯度，而无需通过求解器的操作进行反向传播。由于不存储前向传递中的任何中间量，我们可以在深度的函数下以恒定的内存成本训练我们的模型，这是训练深度模型的一个主要瓶颈。

**自适应计算**  
欧拉方法可能是解决ODE的最简单方法。此后，已经有超过120年的时间用于开发高效且准确的ODE求解器（Runge，1895；Kutta，1901；Hairer等，1987）。现代ODE求解器提供关于近似误差增长的保证，监控误差水平，并动态调整其评估策略，以达到请求的精度。这使得模型评估的成本能够根据问题的复杂度进行扩展。训练后，精度可以根据实时或低功耗应用的需求进行降低。

**参数效率**  
当隐藏单元的动态被参数化为时间的连续函数时，附近“层”的参数会自动关联起来。在第三节中，我们展示了这如何减少监督学习任务所需的参数数量。

**可扩展且可逆的归一化流**  
连续变换的一个意外副作用是，变量变换公式变得更容易计算。在第四节中，我们推导了这个结果，并利用它构建了一类新的可逆密度模型，避免了归一化流中的单一单元瓶颈，且可以通过最大似然直接训练。

**连续时间序列模型**  
与需要对观察和发射间隔进行离散化的递归神经网络不同，连续定义的动态可以自然地结合在任意时间到达的数据。在第五节中，我们构建并展示了这样一个模型。


**图1**  
左：一个残差网络定义了一个离散的有限变换序列。  
右：一个ODE网络定义了一个向量场，持续不断地变换状态。  
两者：圆圈表示评估位置。
![](https://pic1.imgdb.cn/item/67c67c0ad0e0a243d40b34ff.png)


## 2. ODE解的反向自动微分 (Reverse-mode automatic differentiation of ODE solutions)

训练连续深度网络的主要技术难题是在ODE求解器中执行反向自动微分（也称为反向传播）。通过前向传递的操作进行微分是直接的，但会带来高内存成本并引入额外的数值误差。

我们将ODE求解器视为黑箱，并使用伴随灵敏度方法（Pontryagin等，1962）计算梯度。该方法通过在时间上反向求解一个第二个扩展的ODE来计算梯度，适用于所有ODE求解器。该方法随着问题规模线性扩展，具有低内存成本，并能明确控制数值误差。

考虑优化一个标量值损失函数  $L()$ ，其输入是ODE求解器的结果：

$$
L(z(t_1)) = L \left( z(t_0) + \int_{t_0}^{t_1} f(z(t), t, \theta) dt \right) = L \left( \text{ODESolve}(z(t_0), f, t_0, t_1, \theta) \right) \tag{3}

$$

为了优化  $L$ ，我们需要计算关于  $\theta$  的梯度。第一步是确定损失函数如何依赖于每个时刻的隐藏状态  $z(t)$ 。这个量被称为伴随变量，其动态由另一个ODE给出，可以看作是链式法则的瞬时类比：

$$
\frac{da(t)}{dt} = -a(t)^\top \frac{\partial f(z(t), t, \theta)}{\partial z} \tag{4}
$$

我们可以通过另一次调用ODE求解器来计算  $\frac{\partial L}{\partial z(t_0)}$ 。该求解器必须向后运行，从初始值  $z(t_0)$  开始计算  $\frac{\partial L}{\partial z(t_0)}$ 。有一个复杂性是，求解这个ODE需要知道沿着整个轨迹  $z(t)$  的值。然而，我们可以简单地重新计算  $t(t)$  向后运行，并与伴随一起从最终值  $z(t_1)$  开始。

计算相对于参数  $\theta$  的梯度需要评估一个三重积分，该积分依赖于  $z(t)$  和  $a(t)$ ：

$$
\frac{dL}{d\theta} = \int_{t_1}^{t_0} a(t)^\top \frac{\partial f(z(t), t, \theta)}{\partial \theta} dt  \tag{5}
$$

向量-雅可比矩阵乘积  $a(t)^\top \frac{\partial f}{\partial z}$  和  $a(t)^\top \frac{\partial f}{\partial \theta}$ （在公式（4）和（5）中）可以通过自动微分有效地评估，计算成本与评估  $f$  相似。所有用于求解  $z$ 、 $a$  和  $\frac{\partial L}{\partial \theta}$  的积分可以通过一次ODE求解器调用计算，该调用将原始状态、伴随变量和其他部分导数合并为一个向量。算法1展示了如何构建所需的动态，并调用ODE求解器一次性计算所有梯度。


----------

**算法 1** ODE初值问题的反向模式导数

**输入**：动态参数  $\theta$ ，起始时间  $t_0$ ，终止时间  $t_1$ ，最终状态  $z(t_1)$ ，损失梯度  $\frac{\partial L}{\partial z(t_1)}$ 

$$
s_0 = [z(t_1), \frac{\partial L}{\partial z(t_1)}, 0]
$$
 
$$
\text{def aug\_dynamics}([z(t), a(t), :], t, \theta): \quad \text{定义增强状态的动态}
$$
 
$$
\text{return} [f(z(t), t, \theta), -a(t)^\top \frac{\partial f}{\partial z}, -a(t)^\top \frac{\partial f}{\partial \theta}]
$$
 
$$
[z(t_0), \frac{\partial L}{\partial z(t_0)}, \frac{\partial L}{\partial \theta}] = \text{ODESolve}(s_0, aug\_dynamics, t_1, t_0, \theta)
$$

**返回**：

$$
\frac{\partial L}{\partial z(t_0)}, \frac{\partial L}{\partial \theta}
$$

---------

大多数ODE求解器都有在多个时刻输出状态  $z(t)$  的选项。当损失依赖于这些中间状态时，反向模式导数必须分解成一系列单独的求解，每一对连续输出时间之间都有一次求解（图2）。在每次观察时，伴随变量必须调整为相应部分导数  $\frac{\partial L}{\partial z(t_i)}$  的方向。

上述结果扩展了Stapor等人（2018年，第2.4.2节）的研究。扩展版算法1，包括相对于  $t_0$  和  $t_1$  的导数，可在附录C中找到。详细推导见附录B。附录D提供了Python代码，用于计算所有导数，依赖于 scipy.integrate.odeint，通过扩展autograd自动微分包实现。该代码还支持所有更高阶的导数。我们已发布了一个PyTorch（Paszke等，2017年）实现，包括GPU加速的多个标准ODE求解器实现，网址为：github.com/rtqichen/torchdiffeq。

**图2：** 常微分方程解的反向模式微分。伴随灵敏度方法通过时间反向求解增广的常微分方程。增广系统同时包含原始状态和损失对状态的灵敏度。如果损失在多个观测时间点上直接依赖于状态，那么伴随状态必须沿损失相对于每个观测的偏导数方向更新
![](https://pic1.imgdb.cn/item/67c67d9bd0e0a243d40b355c.png)


## 3\. 用ODE替代残差网络进行监督学习

在这一节中，我们实验性地研究了神经ODE在监督学习中的训练。

**Software**   为了数值求解ODE初值问题，我们使用了隐式的Adams方法，该方法在LSODE和VODE中实现，并通过`scipy.integrate`包进行接口。作为一种隐式方法，它比显式方法（如Runge-Kutta）具有更好的保证，但需要在每个步骤中解决一个非线性优化问题。这种配置使得直接的反向传播通过积分器变得困难。我们在Python的`autograd`框架中实现了伴随灵敏度方法（Maclaurin等，2015）。在本节的实验中，我们评估了GPU上使用TensorFlow计算的隐藏状态动态及其导数，这些计算通过Python的`autograd`代码从Fortran的ODE求解器调用。

**Model Architectures**  我们实验了一个小型残差网络，该网络将输入下采样两次，然后应用6个标准的残差块（He等，2016b），这些残差块在ODE-Net变体中被ODE求解模块替代。我们还测试了一个具有相同架构的网络，但梯度直接通过Runge-Kutta积分器反向传播，这个网络被称为RK-Net。表1展示了测试误差、参数数量和内存成本。L表示ResNet中的层数，而  $\tilde{L}$  是ODE求解器在单次前向传播中请求的函数评估次数，可以解释为隐式层数。

我们发现ODE-Net和RK-Net能够实现与ResNet相近的性能，同时使用更少的参数。作为参考，一个具有300个单元的单隐层神经网络的参数数量大致与我们测试的ODE-Net和RK-Net架构相同。


-----------

**表1：MNIST上的表现†来源：LeCun等人（1998）。**

| 模型 | 测试误差 | 参数数量 | 内存 | 时间 |
| --- | --- | --- | --- | --- |
| 1层MLP† | 1.60% | 0.24 M | \- |  $O(L)$  |
| ResNet | 0.41% | 0.60 M |  $O(L)$  |  $O(L)$  |
| RK-Net | 0.47% | 0.22 M |  $O(L)$  |  $O(L)$  |
| ODE-Net | 0.42% | 0.22 M |  $O(1)$  |  $O(L)$  |

-----------

**ODE-Net中的误差控制**  ODE求解器可以大致确保输出在给定的容差范围内接近真实解。改变这个容差会改变网络的行为。我们首先在图3a中验证了误差确实可以被控制。前向调用所花费的时间与函数评估次数成正比（图3b），因此调节容差为我们提供了精度与计算成本之间的权衡。在训练时可以使用高精度，但在测试时切换到较低的精度。

------


![](https://pic1.imgdb.cn/item/67c67ef3d0e0a243d40b35ac.png)
**图3：训练好的ODE-Net的统计信息。（NFE = 函数评估次数）**

图3c显示了一个令人惊讶的结果：反向传递中的评估次数大约是前向传递的一半。这表明，伴随灵敏度方法不仅在内存上更高效，而且比直接通过积分器进行反向传播更具计算效率，因为后者方法需要通过前向传递中的每次函数评估进行反向传播。

------


**网络深度**  尚不清楚如何定义ODE解的“深度”。一个相关的量是所需的隐藏状态动态评估次数，这是一个委托给ODE求解器的细节，且取决于初始状态或输入。图3d显示，函数评估次数在整个训练过程中增加，显然是在适应模型复杂度的增加。


## 4\. 连续归一化流

离散化方程（1）在归一化流（Rezende 和 Mohamed，2015）和NICE框架（Dinh等，2014）中也出现。这些方法使用变量变化定理来计算通过双射函数  $f$  转换样本时概率的精确变化：

$$
z_1 = f(z_0) \quad \Rightarrow \quad \log p(z_1) = \log p(z_0) - \log \left| \frac{\partial f}{\partial z_0} \right| \tag{6}
$$

一个例子是平面归一化流（Rezende 和 Mohamed，2015）：

$$
z(t+1) = z(t) + uh(w^\top z(t) + b), \quad \log p(z(t+1)) = \log p(z(t)) - \log \left| 1 + u^\top \frac{\partial h}{\partial z} \right| \tag{7}
$$

通常，使用变量变化公式的主要瓶颈在于计算Jacobian的行列式  $\frac{\partial f}{\partial z}$ ，它在  $z$  的维度或隐藏单元数量上具有立方成本。最近的研究探讨了归一化流层的表达能力与计算成本之间的权衡（Kingma等，2016；Tomczak 和 Welling，2016；Berg等，2018）。

令人惊讶的是，从离散层的集合到连续变换简化了归一化常数变化的计算：

**定理1（变量瞬时变化）** . 设  $z(t)$  是一个有限的连续随机变量，其概率  $p(z(t))$  随时间变化。令  $\frac{dz}{dt} = f(z(t))$  为描述  $z(t)$  的连续时间变换的微分方程。假设  $f$  在  $z$  上是均匀Lipschitz连续且在  $t$  上连续的，那么概率  $\log p(z(t))$  的变化也服从一个微分方程：

$$
\frac{\partial \log p(z(t))}{\partial t} = -\text{tr} \left( \frac{d f}{d z(t)} \right) \tag{8}
$$

附录A中给出了证明。与（6）中的对数行列式不同，我们现在只需要一个迹操作(trace operation)。与标准有限差分不同，微分方程  $f$  不需要是双射的，因为如果满足唯一性条件，则该变换自动是双射的。



作为变量瞬时变化的一个应用示例，我们可以检查平面流的连续类比及其归一化常数的变化：

$$
\frac{dz(t)}{dt} = uh(w^\top z(t) + b), \quad \frac{\partial \log p(z(t))}{\partial t} = -u^\top \frac{\partial h}{\partial z(t)} \tag{9}
$$

给定初始分布  $p(z(0))$ ，我们可以通过求解这个组合ODE从  $p(z(t))$  中采样并评估其密度。

**使用多个隐藏单元并线性计算成本** . 虽然行列式  $\det$  不是线性函数，但迹函数是线性的，这意味着  $\text{tr}(\sum_n J_n) = \sum_n \text{tr}(J_n)$ 。因此，如果我们的动态是由一组函数之和给出的，那么对数密度的微分方程也是一个和：

$$
\frac{dz(t)}{dt} = \sum_{n=1}^{M} f_n(z(t)), \quad \frac{d \log p(z(t))}{dt} = \sum_{n=1}^{M} \text{tr} \left( \frac{\partial f_n}{\partial z} \right) \tag{10}
$$

这意味着我们可以便宜地评估具有许多隐藏单元的流模型，且其计算成本仅与隐藏单元数量  $M$  线性相关。评估这些“宽”流层使用标准归一化流模型的成本为  $O(M^3)$ ，这意味着标准的NF架构使用多个层，但每层只包含一个隐藏单元。

**时间依赖的动态** . 我们可以将流的参数指定为时间  $t$  的函数，从而使得微分方程  $f(z(t), t)$  随着  $t$  变化，这是一种超网络的参数化方法（Ha等，2016）。我们还为每个隐藏单元引入一个门控机制， $\frac{dz}{dt} = \sum_n \sigma_n(t) f_n(z)$ ，其中  $\sigma_n(t) \in (0,1)$  是一个神经网络，它学习在何时应用动态  $f_n(z)$ 。我们称这些模型为连续归一化流（CNF）。


### 4.1 使用连续归一化流的实验 (Experiments with Continuous Normalizing Flows)

我们首先比较了连续和平面流在学习从已知分布采样方面的表现。我们证明了，具有  $M$  个隐藏单元的平面CNF在表达能力上至少与具有  $K = M$  层的平面NF一样强，有时甚至更加具有表现力。

**密度匹配** 。 我们如上所述配置CNF，并使用Adam（Kingma和Ba，2014）训练10,000次迭代。相比之下，NF使用RMSprop（Hinton等，2012）训练了500,000次迭代，正如Rezende和Mohamed（2015）所建议的那样。对于此任务，我们最小化  $KL(q(x) \parallel p(x))$  作为损失函数，其中  $q$  是流模型， $p(\cdot)$  是目标密度。图4显示了CNF通常能实现更低的损失。

-------
![](https://pic1.imgdb.cn/item/67c69e05d0e0a243d40b5491.png)

**图4：标准化流与连续标准化流的比较。** 标准化流的模型容量由其深度（K）决定，而连续标准化流通过增加宽度（M）也能增加容量，从而使其更容易训练。

------



**最大似然训练** 。 连续时间归一化流的一个有用特性是，我们可以以与前向传递大致相同的成本计算反向变换，而归一化流则无法做到这一点。这使我们能够通过执行最大似然估计来训练流模型，用于密度估计任务，最大化  $\mathbb{E}_{p(x)}[\log q(x)]$ ，其中  $q(\cdot)$  使用适当的变量变化定理计算，然后反向操作CNF生成来自  $q(x)$  的随机样本。

对于此任务，我们为CNF使用64个隐藏单元，为NF使用64层堆叠的单隐藏单元。图5显示了学习到的动态。我们没有展示初始的高斯分布，而是展示了经过一小段时间后的变换分布，显示了初始平面流的位置。有趣的是，为了拟合两个圆形分布，CNF旋转了平面流，使得粒子能够均匀地分布成圆形。虽然CNF的变换是平滑且可解释的，但我们发现NF的变换非常不直观，并且该模型在拟合图5b中的两个半月数据集时遇到了困难。

-----
   
![](https://pic1.imgdb.cn/item/67c69e71d0e0a243d40b55c2.png)
**图5：可视化从噪声到数据的转化。** 连续时间标准化流是可逆的，因此我们可以在密度估计任务上进行训练，并且仍然能够高效地从学习到的密度中进行采样。

-----


## 5\. 一种生成性潜在函数时间序列模型

应用神经网络于不规则采样的数据，如医疗记录、网络流量或神经脉冲数据，是一项挑战。通常，观察值被分成固定时长的时间段，且数据以相同方式离散化。这会导致缺失数据和定义不明确的潜在变量问题。缺失数据可以通过生成时间序列模型来解决（Álvarez 和 Lawrence，2011；Futoma等，2017；Mei和Eisner，2017；Soleimani等，2017a）或数据插补方法（Che等，2018）。另一种方法是将时间戳信息与RNN的输入结合（Choi等，2016；Lipton等，2016；Du等，2016；Li，2017）。

我们提出了一种连续时间的生成性方法来建模时间序列。我们的模型通过潜在轨迹表示每一个时间序列。每个轨迹由一个局部初始状态  $z_{t_0}$  和一个在所有时间序列间共享的全局潜在动态确定。给定观察时间  $t_0, t_1, \dots, t_N$  和初始状态  $z_{t_0}$ ，ODE求解器产生  $z_{t_1}, \dots, z_{t_N}$ ，这些描述了每个观察的潜在状态。我们通过采样过程正式定义这个生成模型：

$$
z_{t_0} \sim p(z_{t_0}) \tag{11}
$$
 
$$
z_{t_1}, z_{t_2}, \dots, z_{t_N} = \text{ODESolve}(z_{t_0}, f, \theta_f, t_0, \dots, t_N) \tag{12}
$$
 
$$
x_{t_i} \sim p(x|z_{t_i}, \theta_x) \tag{13}
$$

函数  $f$  是一个时间不变的函数，它接受当前时间步  $z$  的值并输出其梯度： $\frac{\partial z(t)}{\partial t} = f(z(t), \theta_f)$ 。我们通过神经网络对这个函数进行参数化。因为  $f$  是时间不变的，给定任意潜在状态  $z(t)$ ，整个潜在轨迹是唯一确定的。外推这个潜在轨迹允许我们任意向前或向后进行时间预测。

**训练与预测**  我们可以将这个潜在变量模型作为变分自编码器进行训练（Kingma 和 Welling，2014；Rezende 等，2014），用于带有序列值的观察数据。我们的识别网络是一个RNN，顺序地消耗数据并进行反向传递，输出  $\phi_{\theta}(z_0 | x_1, x_2, \dots, x_N)$ 。附录E中提供了详细的算法。使用ODEs作为生成模型，我们可以在连续时间线上对任意时间点  $t_1 \dots t_M$  进行预测。

**泊松过程似然** 观察事件发生的事实通常能告诉我们关于潜在状态的信息。例如，一个病人可能在生病时更有可能进行医学检查。事件的发生率可以通过潜在状态的函数来参数化： $p(\text{event at time} | z(t)) = \lambda(z(t))$ 。给定这个速率函数，独立观察集在时间区间  $[t_{\text{start}}, t_{\text{end}}]$  内的似然性可以由一个非齐次泊松过程给出（Palm，1943）：

$$
\log p(t_1 \dots t_N | t_{\text{start}}, t_{\text{end}}) = \sum_{i=1}^{N} \log \lambda(z(t_i)) - \int_{t_{\text{start}}}^{t_{\text{end}}} \lambda(z(t)) dt
$$

我们可以使用另一个神经网络来参数化  $\lambda(\cdot)$ 。方便的是，我们可以在一次ODE求解器调用中同时评估潜在轨迹和泊松过程的似然性。图7展示了这样一个模型在一个玩具数据集上学到的事件发生率。

观察时间上的泊松过程似然性可以与数据似然性结合，以联合建模所有观察值及其发生的时间。


------
![](https://pic1.imgdb.cn/item/67c6ada2d0e0a243d40b7b95.png)

图 6: Computation graph of the latent ODE model.


------

------
![](https://pic1.imgdb.cn/item/67c6ade9d0e0a243d40b7bb9.png)

图 7：使用泊松过程似然拟合潜在常微分方程动态模型。点表示事件时间，线表示学习到的泊松过程强度 λ(t)

------


## 5.1 时间序列潜在ODE实验

我们研究了潜在ODE模型在拟合和外推时间序列方面的能力。识别网络是一个具有25个隐藏单元的RNN。我们使用了一个4维的潜在空间。我们用一个包含20个隐藏单元的单隐藏层网络来参数化动态函数  $f$ 。计算  $p(x_{t_i} | z_{t_i})$  的解码器是另一个具有20个隐藏单元的单隐藏层神经网络。我们的基准是一个带有25个隐藏单元的递归神经网络，训练目标是最小化负高斯对数似然。我们训练了第二个版本的RNN，其输入与到下一个观察的时间差连接，以帮助处理不规则观察的RNN。

**双向螺旋数据集**   我们生成了1000个二维螺旋的数据集，每个螺旋从不同的起点开始，在100个等间距的时间步骤上采样。数据集包含两种类型的螺旋：一半是顺时针方向，另一半是逆时针方向。为了使任务更加真实，我们向观察值中添加了高斯噪声。

**具有不规则时间点的时间序列**   为了生成不规则的时间戳，我们随机从每个轨迹中采样点（n = {30, 50, 100}），且不重复。我们报告了在超过训练时使用的时间点的100个时间点上的预测均方根误差（RMSE）。表2显示潜在ODE的预测RMSE明显较低。图8展示了30个子采样点的螺旋重建示例。潜在ODE的重建是通过从潜在轨迹的后验分布中采样并解码到数据空间得到的。附录F中展示了不同时间点数量的示例。我们观察到，不论观察点的数量如何，且尽管存在噪声，重建和外推都与真实值一致。

**潜在空间插值**   图8c展示了投影到潜在空间前两维的潜在轨迹。这些轨迹形成两个独立的轨迹簇，一个解码为顺时针螺旋，另一个解码为逆时针螺旋。图9展示了潜在轨迹作为初始点  $z(t_0)$  的函数平滑变化的情况，从顺时针螺旋切换到逆时针螺旋。

------
![](https://pic1.imgdb.cn/item/67c6af58d0e0a243d40b8034.png)

图8：(a)：通过递归神经网络重建和外推具有不规则时间点的螺旋线。(b)：通过潜在神经常微分方程进行重建和外推。蓝色曲线表示模型预测，红色表示外推。(c)：将推断的四维潜在常微分方程轨迹投影到其前两个维度。颜色表示相应轨迹的方向。模型已经学习到了潜在的动态特性，可以区分这两个方向。

-----------




-----

![](https://pic1.imgdb.cn/item/67c6b06ad0e0a243d40b828f.png)
图9：通过改变 $z_{t_0}$ 的一个维度解码的数据空间轨迹。颜色表示时间的进展，从紫色开始，到红色结束。请注意，左侧的轨迹是逆时针的，而右侧的轨迹是顺时针的

-----



------

**Table 2: Predictive RMSE on test set**

| # Observations | 30/100 | 50/100 | 100/100 |
|----------------|--------|--------|---------|
| RNN            | 0.3937 | 0.3202 | 0.1813  |
| Latent ODE     | 0.1642 | 0.1502 | 0.1346  |

------

## 6\. 范围和局限性

**小批量处理**   与标准神经网络相比，小批量的使用并不那么直接。我们仍然可以通过将每个批次元素的状态连接起来，在ODE求解器中批量评估，从而创建一个维度为  $D \times K$  的组合ODE。在某些情况下，控制所有批次元素的误差可能需要比单独求解每个系统多进行  $K$  次评估。然而，实际上使用小批量时，评估次数并没有显著增加。

**唯一性**   什么时候连续动态具有唯一解？Picard存在性定理（Coddington和Levinson，1955）指出，如果微分方程在  $z$  上是均匀Lipschitz连续的，并且在  $t$  上是连续的，那么初值问题的解存在且唯一。如果神经网络具有有限的权重并且使用Lipschitz非线性函数，如tanh或ReLU，那么这个定理适用于我们的模型。

**设置容差**   我们的框架允许用户在速度和精度之间进行权衡，但在训练过程中需要用户为前向和反向传递选择误差容差。对于序列建模，默认使用  $1.5 \times 10^{-8}$  的容差。在分类和密度估计实验中，我们能够将容差分别减少到  $1 \times 10^{-3}$  和  $1 \times 10^{-5}$ ，且不会降低性能。


**重建前向轨迹**   通过反向运行动态来重建状态轨迹可能会引入额外的数值误差，如果重建的轨迹与原始轨迹发生偏离。这个问题可以通过检查点技术来解决：在前向传递过程中存储  $z$  的中间值，并通过从这些点重新积分来重建精确的前向轨迹。我们没有发现这是一个实际问题，并且我们非正式地检查了，使用默认容差反向运行多个连续归一化流层时，可以恢复初始状态。


## 7\. 相关工作

使用伴随方法训练连续时间神经网络在以前的研究中已有提出（LeCun等，1988；Pearlmutter，1995），但并未在实践中演示。残差网络的解释（He等，2016a）作为近似ODE求解器激发了ResNets中的研究，重点是利用可逆性和近似计算（Chang等，2017；Lu等，2017）。我们通过直接使用ODE求解器以更广泛的方式演示了这些相同的特性。

**自适应计算**   通过训练次级神经网络来选择递归或残差网络的评估次数，可以调整计算时间（Graves，2016；Jernite等，2016；Figurnov等，2017；Chang等，2018）。然而，这在训练和测试时间中引入了额外的开销，并且需要调整的额外参数。相比之下，ODE求解器经过良好研究，具有可扩展的规则来调整计算量。

**通过可逆性进行常数内存反向传播**   最近的研究开发了残差网络的可逆版本（Gomez等，2017；Haber和Ruthotto，2017；Chang等，2017），这与我们的方法提供相同的常数内存优势。然而，这些方法要求有受限的架构，需要划分隐藏单元。我们的方法没有这些限制。

**学习微分方程**   许多工作提出了从数据中学习微分方程的方法。可以训练前馈或递归神经网络来近似微分方程（Raissi和Karnidakis，2018；Raissi等，2018a；Long等，2017），以及应用如流体仿真（Wiewel等，2018）。还有大量工作集中在连接高斯过程（GP）和ODE求解器（Schober等，2014）。GPs已被用于拟合微分方程（Raissi等，2018b）并且可以自然地建模连续时间效应和干预（Soleimani等，2017b；Schulam和Saria，2017）。Ryder等（2018）使用变分推断恢复给定随机微分方程的解。

**通过ODE求解器进行微分**   dolfin库（Farrell等，2013）实现了伴随计算用于一般的ODE和PDE求解，但只能通过反向传播到单个求解器。Stan库（Carpenter等，2015）实现了通过ODE解决方案进行灵敏度分析的模型估计。前向灵敏度分析在变量数目上是二次增长的，而伴随灵敏度分析是线性的（Carpenter等，2015；Zhang和Sandu，2014）。Melicher等（2017）使用伴随方法来训练定制的动态模型。

相比之下，通过提供一个通用的向量-雅可比矩阵，我们允许一个ODE求解器被端到端地训练，同时与其他可微分组件一起使用。在控制中，伴随方法已经被探索（Andersson，2013；Andersson等，In Press，2018），我们展现了黑箱ODE求解器的伴随方法的全面集成到自动微分中（Baydin等，2018）用于深度学习和生成建模。

## 8\. 结论

我们研究了将黑箱ODE求解器作为模型组件的使用，开发了用于时间序列建模、监督学习和密度估计的新模型。这些模型可以自适应地进行评估，并允许明确控制计算速度与精度之间的权衡。最后，我们推导了变量变化公式的瞬时版本，并开发了可扩展到大层数的连续时间归一化流。
